{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fitting-silver",
   "metadata": {},
   "outputs": [],
   "source": [
    "import answer5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-christianity",
   "metadata": {},
   "source": [
    "# Quiz 1. \n",
    "### 다음 문제 중 옳은 것엔 O, 틀린 것엔 X로 표시하시오(띄어쓰기 없이 순서대로 작성)\n",
    "\n",
    "#### 1. Sigmoid가 zero-centered 하지 않아서, gradient update에 비효율적이다.(O/X)\n",
    "#### 2. 학습률이 클 때 dead ReLU가 발생한다.(O/X)\n",
    "#### 3. Data processing을 통해 layers 전체의 zero-mean 을 해결할 수 있다.(O/X)\n",
    "#### 4. 학습데이터와 평가데이터만큼은 분명하게 나눠야 한다.(O/X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embedded-impact",
   "metadata": {},
   "source": [
    "# Quiz 2. \n",
    "#### 1. 크기가 큰 learning rate를 유지 시킬 때 나타나는 특성과, 크기가 작은 learning rate를 유지 시킬 때 나타나는 특성을 적으시오. \n",
    "#### 2. 이 두가지의 모두 특성을 고려하기 위해 나타난 개념이 (a)이다. 이는, 초기에 상대적으로 큰 learning rate을 지정해주고, learning rate이 값이 점차 작아지면서 optima 부근의 좁은 범위로 진입할 수 있게 되는데, 강의에서 소개하는 (a) 방법에는 (b), (c), (d)가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "periodic-international",
   "metadata": {},
   "source": [
    "# Quiz 3\n",
    "\n",
    "#### 1. BatchNomalization 을 써야할 곳을 정하시오. 또한, Deep CNN에서 BN이 필요한 이유를 서술하시오. (입력값 분포 문제, learning-rate와 local minima에 관한 BN 장점을 쓰면 됩니다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-premium",
   "metadata": {},
   "source": [
    "![title](image_for_quiz3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-split",
   "metadata": {},
   "source": [
    "# Quiz 4.\n",
    "##### 그림1\n",
    "![title](image_for_quiz4_1.png)\n",
    "##### 그림2\n",
    "![title](image_for_quiz4_2.jpg)\n",
    "#### 위의 그림1은 batch normalization 과정이 고안된 이유에 대한 그림이다. 초기 입력 레이어를 정규화 하고 난 후, hidden layer를 거치는 과정에서 그 입력이 예측하기 어려운 형태로 변할 수 있다. 이을 정규화 할 수 있다면, 결과물의 성능이 좋아지지 않을까 하는 아이디어에 제안되었다고 한다.\n",
    "#### 그림2는 이 아이디어를 표현한 식이다. 다음 중 옳은 것을 고르시오.\n",
    "##### 1. 하나의 배치에 들어갈 입력데이터의 개수가 300개, 그리고 hidden layer의 입력 차원이 30개일 때, \n",
    "(1) m의 값, \n",
    "\n",
    "(2) update되는 parameter의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-development",
   "metadata": {},
   "source": [
    "# Quiz 5 \n",
    "### 다음 문제 중 옳은 것엔 O, 틀린 것엔 X로 표시하시오(띄어쓰기 없이 순서대로 작성)\n",
    "####  1. ReLU 함수의 장점은 + region에서 saturation이 일어나지 않는다는 것이다 (O/X)\n",
    "#### 2. Sigmoid함수에 zero-mean data를 사용하지 않을 경우, 음수나 양수의 값만 나타날 수 있다 (O/X)\n",
    "#### 3. Batch Normalization은 learning rate를 증가시킨다 (O/X)\n",
    "#### 4. Tanh 함수의 장점은 zero-centered 라는 점이다 (O/X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-lunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-space",
   "metadata": {},
   "source": [
    "# Quiz 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-turtle",
   "metadata": {},
   "source": [
    "### 다음 중 틀린 것을 고르시오!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-scotland",
   "metadata": {},
   "source": [
    "1. Batch-Normalization을 진행하기 위해서는 미니배치의 평균값과 분산을 계산해야 한다. Mini-batch data가 [A, B]의 형태로 주어졌을 때, 평균값과 분산의 shape는 [A,] 이다.\n",
    "2. Test data에도 batch-normalization을 적용하며, 이때 사용하는 평균과 분산은 훈련을 통해 결정된 값들을 사용한다.\n",
    "3. Batch-Normalization으로 인해 가중치 초기값 설정에 대한 의존도가 줄어든다.\n",
    "4. Image processing에서는 PCA나 Whitening은 거의 사용하지 않는다.\n",
    "5. ReLU의 계산속도는 sigmoid보다 빠르기 때문에 activation function으로 많이 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = int(input())\n",
    "\n",
    "answer5.check6(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-conspiracy",
   "metadata": {},
   "source": [
    "# Quiz 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-garlic",
   "metadata": {},
   "source": [
    "활성화 함수로 sigmoid를 쓴다고 가정하자. 이때 입력데이터가 image라면, preprocessing을 해주어야 하는 이유를 쓰시오"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = None\n",
    "answer5.check7(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-portuguese",
   "metadata": {},
   "source": [
    "# Quiz 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-replica",
   "metadata": {},
   "source": [
    "### 다음 문장에서 빈칸에 들어갈 말과, 문장의 o/x 여부를 선택하시오 \n",
    " \n",
    "1. weights initialization이 모두 '0'으로 동일하면, (________)이 발생하지 못한다. \n",
    "\n",
    "2. 활성화 함수가 없어도, 선형 layer를 여러겹 쌓으면 비선형 관계를 구현할 수 있다.\n",
    "\n",
    "3. mini batch 방법을 사용한다고 할 때, Data preprocessing 과정에서 mini batch size에 해당하는 Data의 평균과 표준 편차를 사용해야 한다.\n",
    "\n",
    "(1) symmentric breaking, x, x\n",
    "\n",
    "(2) symmentric controlling, o, x\n",
    "\n",
    "(3) symmentric controlling, o, o\n",
    "\n",
    "(4) asymmetric balancing, x, x\n",
    "\n",
    "(5) asymmentric breaking, o, x\n",
    "\n",
    "(5) asymmentric balancing, o, o\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excited-colonial",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check8()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-preliminary",
   "metadata": {},
   "source": [
    "# Quiz 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-edward",
   "metadata": {},
   "source": [
    "Why do we not use sigmoid and tanh for activation funciton and use ReLU, leaky ReLU instead.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mounted-mason",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check9()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-rendering",
   "metadata": {},
   "source": [
    "# Quiz 10\n",
    "1. 값이 커짐에 따라 포화가 일어날 수 있는 activation function에는 relu함수가 있다.(o,x)\n",
    "2. activation function이 tanh or sigmoid일때, 가중치가 크면 포화가 일어나기 쉽다.(o,x)\n",
    "3. weight initialization 을 해주는 이유는 non-linearity를 유지하기 위함이다 (o,x)\n",
    "4. relu activation function을 사용할 경우 Xabier initialization보다 He initialization이 더 적합하다 (o,x) 이유도 생각해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-content",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer10_ = [None,None,None,None ] \n",
    "answer5.check10(answer10_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-huntington",
   "metadata": {},
   "source": [
    "# Quiz 11\n",
    "1. 데이터를 normalize 하는 이유를 생각해보자.\n",
    "2. 일반적인 이미지 데이터에서는 normalize를 굳이 사용하지 않는 이유는?\n",
    "3. mini batch 단위로 학습 시킬 때는 normalize 시에 mini batch 내의 평균을 사용한다. (O,X)\n",
    "4. batch normalization은 layer의 출력을 unit gaussian으로 만들기 위함이다. (O,X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* 사실 BN은 ICS랑 상관없고 loss의 gradient를 smooth하게 해서 도움이 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer11_ = [None, None] #(3,4번 만)\n",
    "answer5.check11(answer11_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-report",
   "metadata": {},
   "source": [
    "# Quiz 12 \n",
    " 1. 뉴런의 입력이 항상 양수이면 gradient는 전부 양수가 된다.\n",
    " 2. 데이터 전처리를 통해 sigmoid의 zero-mean 문제를 해결할 수 있다.\n",
    " 3. 일반적으로 grid search는 random search에 비해 좋은 결과를 보인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer12_ = input('정답을 입력해주세요. (ex.OOX)  ')\n",
    "answer5.check12(answer12_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-dutch",
   "metadata": {},
   "source": [
    "# Quiz 13 \n",
    "1. sigmoid 함수는 zero-centered가 되어있지 않아 작은 변화에도 매우 민감하다.(o/x)\n",
    "2. 0 이하의 값을 모두 버리는 ReLU 함수의 단점을 보완하기 위해 Leaky ReLU가 나왔다.(o/x)\n",
    "3. Batch normalization은 FC layer의 앞부분에서 해준다.(o/x)\n",
    "4. Batch normalization을 거치면 레이어의 입력이 항상 unit gaussian이 되도록 강제하기 때문에 유연성을 주기 위해서 scaling factor인 $ \\gamma $ 와 $ \\beta $ 를 학습시킨다.(o/x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check13(input(\"정답을 입력해주세요(띄어쓰기 없이, 대소문자 구분x): \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-recall",
   "metadata": {},
   "source": [
    "# Quiz 14\n",
    "(a) sigmoid   (b) tanh   (c) ReLU   (d) Leaky ReLU   (e) PReLU\n",
    "1. saturation이 없는 activation function은?\n",
    "2. 0-centered인 activation function은?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check14_1(input(\"(ex : ab c)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-armenia",
   "metadata": {},
   "source": [
    "3. \n",
    "* (1) ReLU는, x = 0에서 gradient가 정의되지 않는다.\n",
    "* (2) learning rate가 너무 크면 dead ReLU가 발생한다.\n",
    "* (3) 이미지 data processing에서 zero-centered와 normalization을 해야 한다.\n",
    "* (4) batch normalization으로 initialization에 의존을 줄일 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer5.check14_2(input(\"(ex : OOOO)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-tours",
   "metadata": {},
   "source": [
    "# Quiz 15. \n",
    "## 아래의 질문들에 O,X로 답해라. (ex, 정답은 OXOOX 와 같은 형식으로 작성)\n",
    "\n",
    "##### 1. (Activation Function)\n",
    "ReLU 함수는 zero-centered하지 않아서 음수에서 saturation이 된다. (o,x)\n",
    "\n",
    "##### 2. (Data Preprocessing)\n",
    "    Data Processing의 가장 대표적인 방법은 original data를 zero-mean 후 normalize시키는 것인데 image의 경우에는 zero-mean 까지만 한다. (o,x)\n",
    "\n",
    "##### 3. (Weight Initialization)\n",
    "Xavier initialization은 표준정규분포로 뽑은 값을 입력의 수로 scaling한 것을 가중치로 사용하여, 각 layer의 입력을 unit gaussian하게 만든다. (o,x)\n",
    "\n",
    "##### 4. (Batch Normalization)\n",
    "Batch normalization은 강제로 layer의 output이 unit gaussian하게 만드는 것이며 \\begin{equation*} y^{(k)} =  \\gamma^{(k)} * \\hat x^{(k)} + \\beta^{(k)}   \\end{equation*}  식을 사용하여 saturation을 일으킬 정도를 조절할 수 있다. (o,x)\n",
    "\n",
    "##### 5. (Babysitting the Learning Process)\n",
    "regularization을 0으로 설정하고 데이터의 일부만 학습시키면 데이터의 갯수가 적어서 overfitting이 일어나 loss가 매우 작아지는데 이것을 sanity check로 활용하기도 한다. (o,x)\n",
    "\n",
    "##### 6.(Hyperparameter Optimization)\n",
    "learning rate의 최적값들이 우리가 설정한 범위의 경계 부분에 집중되도록 하는 것이 좋다. (o,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-madness",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer15_ = input()\n",
    "answer5.check15(answer15_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
